 ChatGPT, a language model trained by openAI to understand and generate human-like text and answer a variety of questions will soon have a rival competitor named Claude. Claude is an AI system, similar to ChatGPT, developed by Anthropic, a startup co-founded by ex-OpenAI employees that raised over $700 million in funding to date. On December 2022, Anthropic on their Twitter handle announced that they have trained language models to be better at responding to adversarial questions without becoming obtuse and saying very little. According to the tweet, “We do this by conditioning them with a simple set of behavioural principles via a technique called Constitutional AI.” Constitutional AI as answered by Claude is an AI safety research technique developed by researchers at Anthropic, PBC with the goal to train AI systems to be helpful, harmless, and honest using a combination of model self-supervision and other safety methods. However, members of Scale Spellbook, a platform for push-button deployment of prompt-based API endpoints for GPT-3 and large language models, stated that they were able to share some findings and comparison between Claude and ChatGPT after being granted access and updates to Antropolics social media and policies. What you need to know about Claude One of the first tests carried out was asking both Claude and ChatGTP for a brief self-introduction. The first finding after the response was that ChatGTP went straight to the point to answer the question in short summary while Claude gave a detailed explanation in a long sentence. According to the findings, Claude seems to have a detailed understanding of what it is and who its creators are. Also, Scale stated that a test carried out and confirmed by Anthropic, shows that Claude can recall information across 8000 tokens which is more than any publicly known OpenAI. Read also: Nigerians query ChatGPT on ‘How to win Tinubu’, others Calculation For the first comparison, both chatbots were asked to provide an answer to the square root of seven numbers which they both provided. The correct answer to the question was 1555.80. Despite not giving the correct answers, ChatGPT comes up with an impressive answer of 1550 which is very similar to human calculation while Claude came up with 1760, very close to the number but far from the answer. On the second test asking both Chatbots to get the square root of a 12-digit number, it was discovered that Claude was fast to decline a request to answer which shows that it seems to be aware of its inability to answer the above question while ChatGPT went ahead for a trial. Factual Knowledge and reasoning In this series, ChatGPT and Claude were placed with rational questions which require deep thoughts to deliver. In the first series, a newly constructed question was asked regarding the winner of the Super Bowl in the year Justin Bieber was born ChatGPT got the correct answer and also correctly identifies the defeated team, the date of the game, and the score. Claude’s answer was incorrect but close. It identifies the San Francisco 49ers as the winners when they won the Super Bowl one year later in 1995. Other questions were also asked in this series which both got correct. Text summary In this particular series, ChatGTP summarised the text task very well but not in the short paragraph as requested while Claude also summarised well and went further to ask if Its response was satisfactory, offering to make improvements. Other sets of comparisons carried out were Comic writing, Code generation, Mathematical reasoning, and analysis of fictional works, among others. In conclusion, the scale disclosed that Claude is a serious competitor to Chat GPT with improvement in many areas. It noted that Claude feels safer and more fun than ChatGPT adding that its writing is wordier but more realistic. For tasks like calculation and reasoning, ChatGPT and Claude appear similar. Meanwhile, Claude appears to perform very poorly in tasks relating to code generation and code reasoning as it contains mainly bugs and errors while ChatGPT gave a good trial.